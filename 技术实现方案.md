# 技术实现方案 - 详细设计

## 一、数据分析逻辑设计

### 1.1 数据表结构分析

需要先分析以下表格的字段结构：
- **表1（谷歌广告数据）**: 包含广告投放相关数据
- **表2（联盟数据）**: 包含联盟平台相关数据
- **表3（分析结果）**: 表1和表2合并分析后的结果
- **表4、表5**: 数据字典，说明表3中各字段的含义

### 1.2 数据匹配策略

**可能的匹配维度：**
1. **时间维度**: 按日期匹配（前7天的数据）
2. **广告维度**: 按广告ID、关键词、广告组等匹配
3. **效果维度**: 计算点击率、转化率、ROI等指标

**分析流程：**
```
表1（谷歌广告） + 表2（联盟数据）
    ↓
数据清洗和标准化
    ↓
按匹配规则关联
    ↓
计算效果指标
    ↓
生成表3（分析结果）
```

## 二、后端核心代码结构

### 2.1 配置文件 (config.py)
```python
import os
from pydantic_settings import BaseSettings

class Settings(BaseSettings):
    # 数据库配置
    DATABASE_URL: str = "postgresql://user:password@localhost/dbname"
    
    # JWT配置
    SECRET_KEY: str = "your-secret-key"
    ALGORITHM: str = "HS256"
    ACCESS_TOKEN_EXPIRE_MINUTES: int = 1440  # 24小时
    
    # 文件上传配置
    UPLOAD_FOLDER: str = "uploads"
    MAX_FILE_SIZE: int = 10 * 1024 * 1024  # 10MB
    ALLOWED_EXTENSIONS: set = {'.xlsx', '.xls', '.csv'}
    
    # 用户配置
    MANAGER_USERNAME: str = "manager"
    EMPLOYEE_COUNT: int = 10
    
    class Config:
        env_file = ".env"
```

### 2.2 数据模型示例 (models/user.py)
```python
from sqlalchemy import Column, Integer, String, DateTime, Enum
from sqlalchemy.sql import func
from app.database import Base
import enum

class UserRole(str, enum.Enum):
    MANAGER = "manager"
    EMPLOYEE = "employee"

class User(Base):
    __tablename__ = "users"
    
    id = Column(Integer, primary_key=True, index=True)
    username = Column(String(50), unique=True, nullable=False, index=True)
    password_hash = Column(String(255), nullable=False)
    role = Column(Enum(UserRole), nullable=False)
    employee_id = Column(Integer, nullable=True)  # 1-10 for employees
    created_at = Column(DateTime(timezone=True), server_default=func.now())
    updated_at = Column(DateTime(timezone=True), onupdate=func.now())
```

### 2.3 数据分析服务 (services/analysis_service.py)
```python
import pandas as pd
from typing import Dict, List
from app.models.data_upload import DataUpload
from app.models.analysis_result import AnalysisResult

class AnalysisService:
    def __init__(self):
        self.upload_folder = "uploads"
    
    def process_analysis(self, google_ads_file: str, affiliate_file: str, user_id: int) -> Dict:
        """
        处理数据分析：表1 + 表2 -> 表3
        """
        try:
            # 1. 读取表1（谷歌广告数据）
            df_google = self._read_file(google_ads_file)
            
            # 2. 读取表2（联盟数据）
            df_affiliate = self._read_file(affiliate_file)
            
            # 3. 数据清洗
            df_google_clean = self._clean_google_data(df_google)
            df_affiliate_clean = self._clean_affiliate_data(df_affiliate)
            
            # 4. 数据匹配和分析
            result_df = self._merge_and_analyze(df_google_clean, df_affiliate_clean)
            
            # 5. 生成分析结果
            analysis_result = {
                "data": result_df.to_dict('records'),
                "summary": self._calculate_summary(result_df),
                "status": "completed"
            }
            
            return analysis_result
            
        except Exception as e:
            return {"status": "failed", "error": str(e)}
    
    def _read_file(self, file_path: str) -> pd.DataFrame:
        """读取Excel或CSV文件"""
        if file_path.endswith('.csv'):
            return pd.read_csv(file_path, encoding='utf-8-sig')
        else:
            return pd.read_excel(file_path)
    
    def _clean_google_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """清洗谷歌广告数据"""
        # 根据实际表1结构进行清洗
        # 例如：去除空值、格式化日期、标准化列名等
        return df
    
    def _clean_affiliate_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """清洗联盟数据"""
        # 根据实际表2结构进行清洗
        return df
    
    def _merge_and_analyze(self, df_google: pd.DataFrame, df_affiliate: pd.DataFrame) -> pd.DataFrame:
        """
        合并数据并进行分析
        核心计算公式：
        1. 保守EPC = 保守佣金 / 点击
        2. 保守ROI = (保守EPC - CPC) / CPC × 100%
        """
        import numpy as np
        
        # 数据匹配：按日期和广告ID匹配（根据实际字段名调整）
        # 假设表1有字段：日期、广告ID、点击、CPC等
        # 假设表2有字段：日期、广告ID、保守佣金等
        merged = pd.merge(
            df_google,
            df_affiliate,
            on=['日期', '广告ID'],  # 根据实际字段名调整
            how='outer',
            suffixes=('_google', '_affiliate')
        )
        
        # 确保数值字段为数值类型
        merged['点击'] = pd.to_numeric(merged['点击'], errors='coerce').fillna(0)
        merged['CPC'] = pd.to_numeric(merged['CPC'], errors='coerce').fillna(0)
        merged['保守佣金'] = pd.to_numeric(merged['保守佣金'], errors='coerce').fillna(0)
        
        # 计算保守EPC = 保守佣金 / 点击
        merged['保守EPC'] = merged.apply(
            lambda row: row['保守佣金'] / row['点击'] if row['点击'] > 0 else None,
            axis=1
        )
        
        # 计算保守ROI = (保守EPC - CPC) / CPC × 100%
        merged['保守ROI'] = merged.apply(
            lambda row: ((row['保守EPC'] - row['CPC']) / row['CPC'] * 100) 
                       if (row['CPC'] > 0 and row['保守EPC'] is not None) 
                       else None,
            axis=1
        )
        
        # 处理异常值（无穷大和NaN）
        merged['保守EPC'] = merged['保守EPC'].replace([np.inf, -np.inf], None)
        merged['保守ROI'] = merged['保守ROI'].replace([np.inf, -np.inf], None)
        
        # 格式化ROI为百分比（保留2位小数）
        merged['保守ROI'] = merged['保守ROI'].apply(
            lambda x: round(x, 2) if x is not None else None
        )
        
        # 选择需要的列生成表3（根据实际需求调整）
        result_columns = [
            '日期', '广告ID', '点击', 'CPC', '保守佣金', 
            '保守EPC', '保守ROI'
        ]
        
        # 只保留存在的列
        available_columns = [col for col in result_columns if col in merged.columns]
        result_df = merged[available_columns].copy()
        
        return result_df
    
    def _calculate_summary(self, df: pd.DataFrame) -> Dict:
        """计算汇总统计"""
        import numpy as np
        
        summary = {
            "total_rows": len(df),
            "date_range": {
                "start": df['日期'].min() if '日期' in df.columns else None,
                "end": df['日期'].max() if '日期' in df.columns else None
            }
        }
        
        # 计算汇总指标
        if '保守EPC' in df.columns:
            valid_epc = df['保守EPC'].dropna()
            if len(valid_epc) > 0:
                summary['epc'] = {
                    "avg": float(valid_epc.mean()),
                    "max": float(valid_epc.max()),
                    "min": float(valid_epc.min())
                }
        
        if '保守ROI' in df.columns:
            valid_roi = df['保守ROI'].dropna()
            if len(valid_roi) > 0:
                summary['roi'] = {
                    "avg": float(valid_roi.mean()),
                    "max": float(valid_roi.max()),
                    "min": float(valid_roi.min()),
                    "positive_count": int((valid_roi > 0).sum()),  # 盈利的记录数
                    "negative_count": int((valid_roi < 0).sum())   # 亏损的记录数
                }
        
        return summary
```

### 2.4 API路由示例 (api/analysis.py)
```python
from fastapi import APIRouter, Depends, HTTPException, UploadFile, File
from sqlalchemy.orm import Session
from app.database import get_db
from app.middleware.auth import get_current_user
from app.models.user import User
from app.services.analysis_service import AnalysisService
from app.schemas.analysis import AnalysisResultResponse

router = APIRouter(prefix="/api/analysis", tags=["analysis"])

@router.post("/process")
async def process_analysis(
    google_ads_upload_id: int,
    affiliate_upload_id: int,
    current_user: User = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    """触发数据分析"""
    analysis_service = AnalysisService()
    
    # 获取上传的文件路径
    google_upload = db.query(DataUpload).filter(DataUpload.id == google_ads_upload_id).first()
    affiliate_upload = db.query(DataUpload).filter(DataUpload.id == affiliate_upload_id).first()
    
    if not google_upload or not affiliate_upload:
        raise HTTPException(status_code=404, detail="上传记录不存在")
    
    # 执行分析
    result = analysis_service.process_analysis(
        google_upload.file_path,
        affiliate_upload.file_path,
        current_user.id
    )
    
    # 保存结果到数据库
    # ...
    
    return result
```

## 三、前端核心代码结构

### 3.1 API服务 (services/api.js)
```javascript
import axios from 'axios';

const API_BASE_URL = process.env.REACT_APP_API_URL || 'http://localhost:8000';

const api = axios.create({
  baseURL: API_BASE_URL,
  headers: {
    'Content-Type': 'application/json',
  },
});

// 请求拦截器：添加Token
api.interceptors.request.use((config) => {
  const token = localStorage.getItem('token');
  if (token) {
    config.headers.Authorization = `Bearer ${token}`;
  }
  return config;
});

// 响应拦截器：处理错误
api.interceptors.response.use(
  (response) => response,
  (error) => {
    if (error.response?.status === 401) {
      localStorage.removeItem('token');
      window.location.href = '/login';
    }
    return Promise.reject(error);
  }
);

export default api;
```

### 3.2 文件上传组件 (components/Upload/FileUpload.jsx)
```javascript
import React, { useState } from 'react';
import { Upload, Button, message, Select, DatePicker } from 'antd';
import { UploadOutlined } from '@ant-design/icons';
import api from '../../services/api';

const { Option } = Select;

const FileUpload = ({ onUploadSuccess }) => {
  const [uploadType, setUploadType] = useState('google_ads');
  const [dataDate, setDataDate] = useState(null);
  const [uploading, setUploading] = useState(false);

  const handleUpload = async (file) => {
    if (!dataDate) {
      message.error('请选择数据日期');
      return false;
    }

    setUploading(true);
    const formData = new FormData();
    formData.append('file', file);
    formData.append('upload_type', uploadType);
    formData.append('data_date', dataDate.format('YYYY-MM-DD'));

    try {
      const endpoint = uploadType === 'google_ads' 
        ? '/api/upload/google-ads' 
        : '/api/upload/affiliate';
      
      const response = await api.post(endpoint, formData, {
        headers: { 'Content-Type': 'multipart/form-data' },
      });

      message.success('上传成功！');
      onUploadSuccess && onUploadSuccess(response.data);
    } catch (error) {
      message.error('上传失败：' + (error.response?.data?.detail || error.message));
    } finally {
      setUploading(false);
    }

    return false; // 阻止自动上传
  };

  return (
    <div>
      <Select
        value={uploadType}
        onChange={setUploadType}
        style={{ width: 200, marginRight: 16 }}
      >
        <Option value="google_ads">谷歌广告数据（表1）</Option>
        <Option value="affiliate">联盟数据（表2）</Option>
      </Select>

      <DatePicker
        placeholder="选择数据日期"
        onChange={setDataDate}
        style={{ marginRight: 16 }}
      />

      <Upload
        beforeUpload={handleUpload}
        showUploadList={false}
        accept=".xlsx,.xls,.csv"
      >
        <Button icon={<UploadOutlined />} loading={uploading}>
          上传文件
        </Button>
      </Upload>
    </div>
  );
};

export default FileUpload;
```

### 3.3 经理总览页面 (components/Dashboard/ManagerDashboard.jsx)
```javascript
import React, { useEffect, useState } from 'react';
import { Card, Row, Col, Table, Select, DatePicker } from 'antd';
import api from '../../services/api';

const ManagerDashboard = () => {
  const [overviewData, setOverviewData] = useState(null);
  const [employeeData, setEmployeeData] = useState([]);
  const [loading, setLoading] = useState(false);

  useEffect(() => {
    fetchOverviewData();
    fetchEmployeeData();
  }, []);

  const fetchOverviewData = async () => {
    try {
      const response = await api.get('/api/dashboard/overview');
      setOverviewData(response.data);
    } catch (error) {
      console.error('获取总览数据失败', error);
    }
  };

  const fetchEmployeeData = async () => {
    setLoading(true);
    try {
      const response = await api.get('/api/dashboard/employees');
      setEmployeeData(response.data);
    } catch (error) {
      console.error('获取员工数据失败', error);
    } finally {
      setLoading(false);
    }
  };

  const columns = [
    { title: '员工编号', dataIndex: 'employee_id', key: 'employee_id' },
    { title: '用户名', dataIndex: 'username', key: 'username' },
    { title: '上传次数', dataIndex: 'upload_count', key: 'upload_count' },
    { title: '分析次数', dataIndex: 'analysis_count', key: 'analysis_count' },
    { title: '最后上传时间', dataIndex: 'last_upload', key: 'last_upload' },
  ];

  return (
    <div>
      <Row gutter={16} style={{ marginBottom: 24 }}>
        <Col span={6}>
          <Card title="总上传数" bordered={false}>
            {overviewData?.total_uploads || 0}
          </Card>
        </Col>
        <Col span={6}>
          <Card title="总分析数" bordered={false}>
            {overviewData?.total_analyses || 0}
          </Card>
        </Col>
        <Col span={6}>
          <Card title="活跃员工" bordered={false}>
            {overviewData?.active_employees || 0}
          </Card>
        </Col>
        <Col span={6}>
          <Card title="今日上传" bordered={false}>
            {overviewData?.today_uploads || 0}
          </Card>
        </Col>
      </Row>

      <Card title="员工数据总览">
        <Table
          columns={columns}
          dataSource={employeeData}
          loading={loading}
          rowKey="employee_id"
        />
      </Card>
    </div>
  );
};

export default ManagerDashboard;
```

## 四、数据库初始化脚本

### 4.1 创建初始用户
```python
# scripts/init_users.py
from app.database import SessionLocal
from app.models.user import User
from werkzeug.security import generate_password_hash

def init_users():
    db = SessionLocal()
    
    # 创建经理账户
    manager = User(
        username="manager",
        password_hash=generate_password_hash("manager123"),  # 默认密码
        role="manager",
        employee_id=None
    )
    db.add(manager)
    
    # 创建10个员工账户
    for i in range(1, 11):
        employee = User(
            username=f"employee{i}",
            password_hash=generate_password_hash(f"employee{i}123"),  # 默认密码
            role="employee",
            employee_id=i
        )
        db.add(employee)
    
    db.commit()
    db.close()
    print("用户初始化完成！")
```

## 五、部署配置

### 5.1 Docker Compose配置
```yaml
version: '3.8'

services:
  db:
    image: postgres:15
    environment:
      POSTGRES_USER: admin
      POSTGRES_PASSWORD: password
      POSTGRES_DB: google_analysis
    volumes:
      - postgres_data:/var/lib/postgresql/data
    ports:
      - "5432:5432"

  backend:
    build: ./backend
    command: uvicorn app.main:app --host 0.0.0.0 --port 8000
    volumes:
      - ./backend:/app
      - ./uploads:/app/uploads
    ports:
      - "8000:8000"
    depends_on:
      - db
    environment:
      DATABASE_URL: postgresql://admin:password@db:5432/google_analysis

  frontend:
    build: ./frontend
    ports:
      - "3000:3000"
    depends_on:
      - backend
    environment:
      REACT_APP_API_URL: http://localhost:8000

  nginx:
    image: nginx:alpine
    ports:
      - "80:80"
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf
    depends_on:
      - backend
      - frontend

volumes:
  postgres_data:
```

## 六、开发注意事项

### 6.1 数据分析规则
- **关键步骤**: 需要仔细分析表1、表2、表3的实际字段
- **匹配逻辑**: 确定如何将表1和表2的数据关联起来
- **计算规则**: 确定表3中各指标的计算公式

### 6.2 性能优化
- 大文件处理：使用分块读取
- 异步任务：使用Celery处理耗时的分析任务
- 缓存：Redis缓存常用查询结果

### 6.3 错误处理
- 文件格式验证
- 数据完整性检查
- 异常情况记录和通知

### 6.4 用户体验
- 上传进度显示
- 分析进度提示
- 友好的错误提示
- 数据可视化图表

